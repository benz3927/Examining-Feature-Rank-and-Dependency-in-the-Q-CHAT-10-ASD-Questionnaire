---
format: html
linestretch: 1
crossref:
  lof-title: "List of Figures"
toc: false
bibliography: references.bib
csl: electronic-journal-of-statistics.csl
editor: 
  markdown: 
    wrap: 72
---

### Examining Feature Rank and Dependency in the Q-CHAT-10 ASD Questionnaire

### Abstract

Recent research has found consistent and significant increases in the
prevalence of Autism Spectrum Disorder (ASD) diagnoses for children
since the early 2000s. A widely used screening survey is the Q-CHAT-10
checklist questionnaire. We examine 1054 Q-CHAT-10 responses for
toddlers collected by Dr. Fadi Thabtah using random forest, chi-squared
tests, and randomized permutation to identify dependency among the
questions. From our chi-square tests, we observe a high level of
dependency among the similarly ranked most important variables, a
moderate level of dependency among similarly low-ranked variables, and
no statistically significant relationship between top and bottom groups
of variables. To improve the Q-CHAT-10 survey methodology, we suggest
selecting the most representative questions out of similarly-ranked high
and low-importance questions while developing research on new survey
questions, which may be able to help with early ASD identification.

### Background and Significance

Social and developmental disabilities place a significant psychological
and economic burden on individuals affected by ASD as well as their
families, schools and healthcare systems [@lavelle2014a]. The increasing
prevalence of ASD is likely due to a variety of causes including
increasing rates of diagnosis, greater degree of sensitivity in
diagnostic criteria, shifting demographics, and/or biological changes.
Accurate and early screening is crucial, and allows time and resources
to be allocated in a more effective way. Early identification of ASD may
improve the efficacy of treatment and reduce the long-term economic
burdens of families. Recent technological advancements have paved the
way for modern statistical learning techniques to be used in screening
for multiple diseases, including ASD [@eslami2021].

Our study uses open source data from 1054 Q-CHAT-10 toddler
questionnaires collected by Dr. Thabtah, from the Nelson Marlborough
Institute of Technology via the ASD test app in 2018 [@thabtah2018data].
We used random forests to identify the largest contributing variables
involved in the Q-CHAT-10 questionnaire for toddlers[^1] and permutation
tests to explore the dependency between questions.

[^1]: The Q-CHAT-10 questionnaire is a survey that checks for red flags
    for ASD screening. The scoring is as follows: "For questions 1-9: if
    you circle an answer in columns C, D or E, score 1 point per
    question. For question 10: if you circle an answer in columns A, B
    or C, score 1 point. Add points together for all ten questions. If
    your child scores 3 or above, the health professional may consider
    referring your child for a multi-disciplinary assessment" [@qchat].

### Methods

We use Tidymodels @tidymodels in R @R to tune a random forest model. Our
model produces a ranking of questions by relative importance in
predicting an overall ASD screening result.[^2] Based on our variable
importance ranking, we conduct several chi-square tests with permuted
samples and test dependency between several of the top-ranked questions.
We also conduct a larger chi-square test in which the two groups are
4000 bootstrap samples from the top 5 questions and 4000 bootstrap
samples from the bottom 5 questions. We tested if top and bottom groups
were independent of each other. We conduct multiple hypothesis tests so
we set an adjusted alpha level of 0.001 / (# of tests) = 0.001 / (10) =
0.0001. With such a low significance level, we are taking into account
the fact that with each additional test we conduct, there's a greater
chance of falsely rejecting the null hypothesis (Type I error). An 80/20
training/testing split was used to fit our random forest model. Within
our training data, 20% is saved as the validation set of our random
forest model @Rf_tutorial.

[^2]: See Figure 2 in Appendix

### Data Overview and Variable Selection

For our random forest model, we use 10 predictors, questions A1 through
A10, out of 18 total variables from the data set. These 10 questions are
binary numerical responses from the survey questions within the
Q-CHAT-10 questionnaire, which indicate if the response represents
ASD-like traits or not [@thabtah2018data]. The response variable of our
model is the final screening result, Class.ASD.Trait, a categorical
variable with two levels: "yes" and "no". Yes corresponds to a
questionnaire score of 4 or more. There are several other variables such
as ethnicity, jaundice, and gender, etc. which we do not use in our
model.

### Results

We observe that the most optimal random forest model has an mtry (number
of randomly sampled predictors at each tree split) of 1 and a minimum
number of data points at each node of 26. Our final tuned random forest
model yielded the following confusion matrix on the testing data.

#### Tuned Random Forest on Testing Results - Confusion Matrix {.appendix}

|                  | Observed (no) | Observed (yes) |
|------------------|---------------|----------------|
| Prediction (no)  | 57            | 0              |
| Prediction (yes) | 9             | 146            |

Sensitivity = $\frac{TP}{TP + FN}= \frac{146}{146 + 0}=1$, False
Negative rate (Type II error) = 1- sensitivity = 0

Specificity = $\frac{TN}{TN + FP}=\frac{57}{57 + 9}=0.8636$, False
Positive rate (Type I Error) = 1 - specificity = 0.1364

We observe that our tuned random forest model has an OOB (Out-Of-Bag)
classification error of 4.2% on our testing data. Our model slightly
over-fits the unseen testing data because it is slightly more sensitive
than specific. The trade-off of high sensitivity (100%) is a slightly
higher false positive rate of (13.64%). A higher probability of a Type 1
error is slightly preferable for early diagnosis to a high Type II
probability in screening for ASD in toddlers because it is better for
most individuals to receive a correct positive screening result with the
exception of a few false positives than failing to classify many
ASD-positive individuals @wolff2022a.

Within our Chi-Square tests, we observed statistically significant
relationships between question 9 \~ 6 ($\chi^{2}(1) = 139.49$, p \<
0.0001) and between question 7 \~ 5 ($\chi^{2}(1) = 96.6$, p \< 0.0001).
We observed no statistically significant relationship between the 4000
randomly sampled top questions and 4000 randomly sampled bottom
questions ($\chi^{2}(1) = 0.847$, p \> 0.0001). This top \~ bottom test
validates the general accuracy of the Q-CHAT-10 Checklist because if top
and bottom questions are independent of each other, then questions
mostly vary and are able to pick up on different behavioral traits. In
our contingency tables, we observe that, in general (not in particular
pairing of questions), similarly-ranked top questions are highly
correlated with each other and bottom questions are moderately
correlated with each other.[^3]

[^3]: See Appendix for full mosaic plots

### Discussion & Conclusions

We decided to closely examine several of the top questions because we
expect them to screen for similar traits. Question 6 probes into whether
a child pays attention to where his or her parents are looking at and 9
pertains to whether a child is able to communicate through simple
gestures [@ellawadi2014]. Questions 6 and 9 relate to whether the
toddler pays attention to his or her surroundings and is able to
communicate his or her needs. Questions 5 and 7 respectively screen for
whether the toddler plays pretend or knows and feels the emotions of his
her family if they are upset [@mcdonald2011]. Although there is some
repetition between similarly ranked questions, questions are not
repetitive to the extent that the entire top or bottom group could be
removed: the distribution of screening results between top group and
bottom group are largely independent.

To improve the Q-CHAT-10 survey methodology, we suggest selecting the
most representative questions out of similarly high-ranked question and
selecting the most representative questions out of low-importance
questions. This may reduce redundancy and improve the overall accuracy
of the survey. We conclude that, in addition to the issue of some
redundancy because of the high correlation between top features, more
questions may be added to the questionnaire that are different from
existing ones to increase the robustness of the Q-CHAT-10 survey and
improve its ability to capture social and behavioral ASD-like traits.

### References:

::: {#refs}
:::

### Appendix {.appendix}

Below is an equation for predicting the final ASD screening result based
on the responses to each survey question. Y represents the final
screening result of each toddler: 1 (of concern and should see
professional diagnosis) and 0 (negative). The sum of $x_i$s represents
the sum of the individual scores for each question on the screening
survey, which can be one or zero. The true model of our data is
deterministic. Since whether the toddler screened for ASD traits is
already known, the predictive capabilities of our random forest model
are not central to our study. The variable importance plot from our RF
model, however, is essential to our understanding of feature dependency
and our suggestions for improving the Q-CHAT-10 survey.

$$
 y  =\begin{cases} 
      1 & \sum_{i=1}^{10} x_i \geq4\\
      0 & \sum_{i=1}^{10} x_i  < 4\\
   \end{cases}
$$

```{r include = FALSE}
# Libraries
library(tidymodels)
library(tidyverse)
library(randomForest)
library(ranger)
```

```{r include = FALSE}
# Load Autism screening data for toddlers as csv
remote <- 'https://raw.githubusercontent.com/'
account <- 'benz3927/Data/main/'
folder <- 'Toddler%20Autism%20dataset%20July%'
file <- '202018.csv'
url <- str_c(remote, account, folder, file)
raw_screening <-  tibble(read.csv(file = url))


# remote <- "https://media.githubusercontent.com/media/"
# account <- "turalsadigov/"
# folder <- "MATH_254/main/Datasets%20for%20projects/"
# file <- "Toddler_Autism_dataset_July_2018_Drake_Ben.csv"
# url <- str_c(remote, account, folder, file)
# raw_screening <- read_csv(url)

# Data Wrangling
screening <-
  raw_screening %>% 
  mutate(Class.ASD.Traits = as_factor(Class.ASD.Traits.)) %>%
  select(-Class.ASD.Traits.) %>% 
  mutate(across(where(is.numeric), as.factor)) %>% 
  mutate(across(where(is.character), as.factor)) %>% 
  mutate(Who.completed.the.test = tolower(Who.completed.the.test))
```

```{r, include = FALSE}
# Data Splitting
set.seed(2022)
splits <- initial_split(screening, strata = Class.ASD.Traits, prop = 0.80)

screening_other <- training(splits)
screening_test  <- testing(splits)

# total training set proportions by children
screening_other %>% 
  count(Class.ASD.Traits) %>% 
  mutate(prop = n/sum(n))


# test set proportions by children
screening_test  %>% 
  count(Class.ASD.Traits) %>% 
  mutate(prop = n/sum(n))

# # validation set
val_set <- validation_split(screening_other,
                            strata = Class.ASD.Traits,
                            prop = 0.80)
```

```{r score distribution, fig.cap="Q-CHAT-10 Scores Overall Distribution", echo = FALSE, message=FALSE, warning = FALSE}
raw_screening %>% 
  ggplot(aes(x = Qchat.10.Score, y = ..density..)) +
  geom_histogram(bins = 10, color = 'white', fill = 'darkgreen') + 
  ggtitle('Q-CHAT-10 Score Distribution')
```

We used the Tidyverse package [@tidyverse] to wrangle our data and
plotted all Q-CHAT-10 scores on a histogram. The mode is around a score
of 4. The center/median of the distribution is around 5. The
distribution of Q-CHAT scores is roughly symmetric without significant
outliers. Q-CHAT-10 scores range from 0 to 10.

All collected data is a simple random sample (data is independently,
identically distributed).

```{r include = FALSE}
library(randomForest)
library(infer)
library(tidyverse)
library(vip)
library(tidymodels)
library(vcd)
```

```{r include = FALSE}
set.seed(2022)

cores <- parallel::detectCores()
cores

rf_mod <-
  rand_forest(mtry = tune(), 
              min_n = tune(), 
              trees = 1000) %>%
  set_engine("ranger", 
             num.threads = cores) %>%
  set_mode("classification")

rf_recipe <-
  recipe(Class.ASD.Traits ~ A1 + A2 + A3 + A4 + A5 + A6 + A7 + A8 + A9 + A10, data = screening_other)

rf_workflow <-
  workflow() %>%
  add_model(rf_mod) %>%
  add_recipe(rf_recipe)

extract_parameter_set_dials(rf_mod)
```

```{r include = FALSE}
# finding the best model using roc and auc
rf_resample <-
  rf_workflow %>%
  tune_grid(val_set,
            grid = 25,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(roc_auc))

rf_resample %>% 
  show_best(metric = "roc_auc")

# graph models, why do these graphs look different 
autoplot(rf_resample)

# finding the best manual manually
rf_resample$.metrics

# Finding the best model out of the 25 Random Forest models.
rf_best <- 
  rf_resample %>% 
  select_best(metric = "roc_auc")
rf_best

# at some point the Yes's and No's flipped
rf_resample %>% 
  collect_predictions()

rf_auc <- 
  rf_resample %>% 
  collect_predictions(parameters = rf_best, type = 'response') %>% 
  roc_curve(Class.ASD.Traits, .pred_No) %>% 
  mutate(model = "Random Forest")
```

```{r include = FALSE}
set.seed(2022)
last_rf_mod <-
  rand_forest(mtry = 1, 
              min_n = 26, 
              trees = 1000) %>%
  set_engine("ranger", 
             num.threads = cores, 
             importance = "impurity") %>%
  set_mode("classification")

last_rf_workflow <- 
  rf_workflow %>% 
  update_model(last_rf_mod)

last_rf_fit <- 
  last_rf_workflow %>% 
  last_fit(splits)

last_rf_fit %>% 
  collect_metrics()
```

```{r include = FALSE}
# confusion matrix for tuned model
set.seed(2022)
last_rf_fit_predictions <- 
  last_rf_fit %>% 
  collect_predictions()

last_rf_fit_predictions %>% 
  conf_mat(Class.ASD.Traits, .pred_class) 

save(last_rf_fit, file = 'QCHAT-RandomForest_Last_fit.Rdata')

```

```{r vip_tuned, fig.cap="Variable Importance Plot", echo=FALSE}
last_rf_fit %>% 
  extract_fit_parsnip() %>% 
  vip(num_features = 20) + 
  ggtitle('Most Important Features From Tuned Random Forest Model')
```

```{r roc-auc curve, fig.cap="Receiver Operating Characteristics (ROC) Curve", include = FALSE}
autoplot(rf_auc) +
  ggtitle('Tuned Random Forest ROC/AUC Curve Accuracy')
```

```{r top, fig.cap="Mosaic Plot for Top Questions 9, 7, 6, and 5", echo=FALSE}
# top ranked questions
mosaic( ~ A9 + A7 + A6 + A5, data = screening_other, shade = TRUE)
```

#### Testing Dependency between Top Questions

In Figure 3, above, the highly saturated quadrants in contingency plots
are statistically significant [@MIT]. There are multiple highly
saturated blue and red within the contingency table of the top 4
questions ranked from our feature importance. Several of the top
questions are highly correlated.

```{r bottom, fig.cap="Mosaic Plot for Bottom Questions 8, 4, 3, and 2", echo = FALSE}
mosaic( ~ A2 + A4 + A8 + A3, data = screening_other, shade = TRUE)
```

#### Testing Dependency between Bottom Questions

In Figure 4, there are some highly saturated blue and red cells within
the contingency table of the bottom 4 questions ranked from our feature
importance. Several of the bottom questions are correlated, but there
are a more statistically non-significant grey cells. Although there
seems to be some correlation between bottom questions, this correlation
seems weaker than the correlation between top questions.

```{r include = FALSE}
set.seed(2022)
top<-unlist(screening_other[,1+c(9,7,6,5,1)])   ## Add 1 since the A9 is at the 10th column.
top <- tibble(top) %>% 
  slice_sample(n=4000)


bottom <-unlist(screening_other[,1+c(10,8,4,3,2)])
bottom <- tibble(bottom) %>% 
  slice_sample(n=4000)

df_new <- 
  top %>% 
  bind_cols(bottom)


# glimpse(df_new)

 obs_statistic_2groups <-
   df_new %>%
   specify(top ~ bottom, success = '1') %>%
   calculate(stat = 'Chisq', order = c('1', '0'))


# Group 1 has top 5 features and Group 2 has bottom 5 features
null_dist_2groups <-
  df_new %>%
  specify(top ~ bottom, success = '1') %>%
  hypothesise(null = 'independence') %>%
  generate(reps = 1000, type = 'permute') %>%
  calculate(stat = 'Chisq', order = c('1', '0'))

p_value_2groups <- null_dist_2groups %>%
  get_p_value(obs_stat = obs_statistic_2groups,
              direction = "greater")

p_value_2groups

null_dist_2groups %>%
  visualise() +
  shade_p_value(obs_stat = obs_statistic_2groups,
                direction = 'right')

groups_test = chisq.test(df_new$top, df_new$bottom)
groups_test$expected

obs_statistic_2groups


top_bottom_df <- df_new

```

```{r top~bottom, fig.cap="Mosaic Plot for Top ~ Bottom", echo=FALSE}
mosaicplot(top_bottom_df$top ~ top_bottom_df$bottom, data = top_bottom_df, 
           xlab = 'Bottom 5 Features',ylab = 'Top 5 Features', shade = TRUE)


```

#### Comparing 4000 Random Samples from Aggregated Group of the Top 5 Feature Paired with 4000 Random Samples from an Aggregated Group of the Bottom 5 Features

There are two categorical variables -\> "top" and "bottom" in which
responses are "0" for not of concern and "1" for of concern. There is
independence of observations: each survey response is not related to
another. Large sample size: there's an expected count condition that 80%
of expected cells must be greater than 5 and all expected cells must be
greater than 1. In our top \~ bottom data, every expected value in each
cell is greater than 200, so the conditions for the chi-square test are
met.

Our contingency plot in Figure 5 agrees with the results of our
Chi-Square test. All quadrants of our mosaic plot are white, meaning
that in each box, there is very small Pearson residual which is close to
0. Since our p-value (0.341) \> alpha (0.0001), we fail to reject the
null hypothesis that the the distribution of positive screening results
in the top group are independent of the distribution of those in the
bottom group. The distribution of positive screening results in the
top-ranked questions are independent of those in the bottom-ranked
questions.

```{r include = FALSE}
# testing question 6 (very highest ranked question) and question 10 (lowest ranked question) there is no statistically significant relationship between question 6 and 10. Therefore, questions of high and low importance must be kept.
set.seed(2022)
observed_chisq_stat_6_10 <- 
  screening_other %>%
  specify(A6 ~ A10, success = '1') %>%
  calculate(stat = "Chisq", order = c('1', '0'))

null_dist_6_10 <- screening_other %>%
  specify(A6 ~ A10, success = '1') %>%
  hypothesize(null = "independence") %>%
  generate(reps = 1000, type = "permute") %>%
  calculate(stat = "Chisq", order = c('1', '0')) 

null_dist_6_10 %>%
  visualize() + 
  shade_p_value(observed_chisq_stat_6_10,
                direction = "greater")
  

p_value_6_10 <- null_dist_6_10 %>%
  get_p_value(obs_stat = observed_chisq_stat_6_10,
              direction = "greater")

observed_chisq_stat_6_10
p_value_6_10

six_two <- chisq.test((table(screening_other$A6, screening_other$A10)))
six_two$expected

mosaic(A6 ~ A10, data = screening_other, shade = TRUE,gp = shading_max)
```

#### Testing Dependency between Question 6 and Question 9

Large sample size: there's an expected count condition that 80% of
expected cells must be greater than 5 and all expected cells must be
greater than 1. In our 9 \~ 6 data, every expected value in each cell is
greater than 100, so the conditions for the chi-square test are met.

Since our observed p-value (2.2e-16) is \< alpha significance level
0.0001, we reject the null hypothesis that the distribution of positive
screening results in question 9 is independent of those in 6.

```{r include = FALSE}
# Looking at specific top questions 9 and 6
set.seed(2022)
observed_chisq_stat_9_6 <- 
  screening_other %>%
  specify(A9 ~ A6, success = '1') %>%
  calculate(stat = "Chisq", order = c('1', '0'))

null_dist_9_6 <- screening_other %>%
  specify(A9 ~ A6, success = '1') %>%
  hypothesize(null = "independence") %>%
  generate(reps = 1000, type = "permute") %>%
  calculate(stat = "Chisq", order = c('1', '0')) 

null_dist_9_6 %>%
  visualize() + 
  shade_p_value(observed_chisq_stat_9_6,
                direction = "greater")
  

p_value_9_6 <- null_dist_9_6 %>%
  get_p_value(obs_stat = observed_chisq_stat_9_6,
              direction = "greater")

observed_chisq_stat_9_6
p_value_9_6

nine_six <- chisq.test((table(screening_other$A9, screening_other$A6)))
nine_six$expected

mosaic(A9 ~ A6, data = screening_other, shade = TRUE,gp = shading_max)

```

#### Testing Dependency Between Question 5 and 7

Large sample size: there's an expected count condition that 80% of
expected cells must be greater than 5 and all expected cells must be
greater than 1. In our 7 \~ 5 data, every expected value in each cell is
greater than 100, so the conditions for the chi-square test are met.

Since our observed p-value (2.2e-16) is \< alpha significance level
0.0001, we reject the null hypothesis that the distribution of positive
screening results in question 7 are independent of those in 5.

```{r include = FALSE}
set.seed(2022)
# Chi-square test for 7 and 5
observed_chisq_stat_7_5 <- 
  screening_other %>%
  specify(A7 ~ A5, success = '1') %>%
  calculate(stat = "Chisq", order = c('1', '0'))

null_dist_7_5 <- screening_other %>%
  specify(A7 ~ A5, success = '1') %>%
  hypothesize(null = "independence") %>%
  generate(reps = 1000, type = "permute") %>%
  calculate(stat = "Chisq", order = c('1', '0')) 

null_dist_7_5 %>%
  visualize() + 
  shade_p_value(observed_chisq_stat_7_5,
                direction = "greater")
  

p_value_7_5 <- null_dist_7_5 %>%
  get_p_value(obs_stat = observed_chisq_stat_7_5,
              direction = "greater")

observed_chisq_stat_7_5
p_value_7_5

seven_five <- chisq.test((table(screening_other$A7, screening_other$A5)))
seven_five$expected

mosaic(A7 ~ A5, data = screening_other, shade = TRUE,gp = shading_max)
```

![Q-CHAT-10 Checklist from the National Institute for Health Research
[@qchat].](QChatChecklist.png){alt="Q-CHAT-10 Checklist from the National Institute for Health Research [@qchat]."
fig-align="center" width="586"}
